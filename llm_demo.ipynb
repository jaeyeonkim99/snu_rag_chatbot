{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gradio as gr\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(input_str):\n",
    "    # change your default input according to your model\n",
    "\n",
    "    # eeve model (yanolja)\n",
    "    inputs = tokenizer(f\"{input_str}\\n\\n### Response: \", return_tensors=\"pt\").to(\"cuda\")\n",
    "    # Mistral & llama 2 model\n",
    "    # inputs = tokenizer('[INST]'+input_str+'[/INST]', return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # inputs = tokenizer(input_str, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            max_new_tokens=1,\n",
    "            top_p=0.6,\n",
    "            top_k=1,\n",
    "            temperature=0.9,\n",
    "            repetition_penalty=1.0,\n",
    "            num_beams=1,\n",
    "            early_stopping=False\n",
    "        )\n",
    "\n",
    "        while int(outputs[0][-1].detach().cpu().numpy()) != tokenizer.eos_token_id:\n",
    "            # yield after [/INST] token, if not using [/INST] token, remove split(\"[/INST]\")[1]\n",
    "            yield tokenizer.batch_decode(outputs.detach().cpu().numpy())[0]#.split(\"[/INST]\")[1]\n",
    "            outputs = model.generate(\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                input_ids=outputs,\n",
    "                max_new_tokens=1,\n",
    "                top_p=0.6,\n",
    "                top_k=1,\n",
    "                temperature=0.9,\n",
    "                repetition_penalty=1.0,\n",
    "                num_beams=1,\n",
    "                early_stopping=False\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:02<00:00,  1.74it/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.86k/1.86k [00:00<00:00, 8.45MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.18M/2.18M [00:00<00:00, 2.33MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 557/557 [00:00<00:00, 2.36MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype = 'auto',\n",
    "    trust_remote_code = True,\n",
    "    # use_cache = True,\n",
    "    # ignore_mismatched_sizes=False,\n",
    "    # low_cpu_mem_usage=True,\n",
    "    # load_in_8bit = True, \n",
    "    ).to(\"cuda\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3876815/879999510.py:3: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  inputs=gr.inputs.Textbox(lines=20, label=\"Input Text\"),\n",
      "/tmp/ipykernel_3876815/879999510.py:3: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  inputs=gr.inputs.Textbox(lines=20, label=\"Input Text\"),\n",
      "/tmp/ipykernel_3876815/879999510.py:3: GradioDeprecationWarning: `numeric` parameter is deprecated, and it has no effect\n",
      "  inputs=gr.inputs.Textbox(lines=20, label=\"Input Text\"),\n",
      "/tmp/ipykernel_3876815/879999510.py:4: GradioDeprecationWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  outputs=gr.outputs.Textbox(label=\"Generated Text\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://5b61f05f1f0bab4112.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://5b61f05f1f0bab4112.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:509: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "demo = gr.Interface(\n",
    "    fn=generate,\n",
    "    inputs=gr.inputs.Textbox(lines=20, label=\"Input Text\"),\n",
    "    outputs=gr.outputs.Textbox(label=\"Generated Text\"),\n",
    "    title=\"LIGHT-INFERENCE\",\n",
    "    description=f\"LLM RAG chatbot demo\",\n",
    ")\n",
    "\n",
    "demo.queue()#max_size=2)\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
